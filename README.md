# A-Simple-neural-network

This is a simple neural network Test

This Demo Use two layer neural network to classify various sample points on two dimensional plane

resultï¼š

iteration 0: loss 1.386415
iteration 100: loss 1.202871
iteration 200: loss 1.148918
iteration 300: loss 0.994547
iteration 400: loss 0.842875
iteration 500: loss 0.756418
iteration 600: loss 0.713461
iteration 700: loss 0.669246
iteration 800: loss 0.639323
iteration 900: loss 0.652191
iteration 1000: loss 0.636914
iteration 1100: loss 0.678593
iteration 1200: loss 0.609264
iteration 1300: loss 0.583731
iteration 1400: loss 0.616781
iteration 1500: loss 0.570278
iteration 1600: loss 0.558064
iteration 1700: loss 0.593524
iteration 1800: loss 0.707873
iteration 1900: loss 0.550037
iteration 2000: loss 0.571608
iteration 2100: loss 0.552941
iteration 2200: loss 0.549366
iteration 2300: loss 0.550949
iteration 2400: loss 0.545961
iteration 2500: loss 0.533732
iteration 2600: loss 0.548755
iteration 2700: loss 0.532853
iteration 2800: loss 0.531948
iteration 2900: loss 0.536291
iteration 3000: loss 0.537323
iteration 3100: loss 0.528223
iteration 3200: loss 0.530325
iteration 3300: loss 0.539718
iteration 3400: loss 0.527770
iteration 3500: loss 0.539795
iteration 3600: loss 0.541734
iteration 3700: loss 0.541370
iteration 3800: loss 0.539819
iteration 3900: loss 0.525085
iteration 4000: loss 0.553415
iteration 4100: loss 0.531996
iteration 4200: loss 0.537491
iteration 4300: loss 0.550671
iteration 4400: loss 0.523990
iteration 4500: loss 0.547799
iteration 4600: loss 0.536805
iteration 4700: loss 0.530245
iteration 4800: loss 0.542115
iteration 4900: loss 0.531899
iteration 5000: loss 0.890184
iteration 5100: loss 0.534059
iteration 5200: loss 0.527247
iteration 5300: loss 0.535971
iteration 5400: loss 0.556964
iteration 5500: loss 0.525992
iteration 5600: loss 0.522328
iteration 5700: loss 0.553893
iteration 5800: loss 0.530625
iteration 5900: loss 0.523883
iteration 6000: loss 0.526772
iteration 6100: loss 0.681123
iteration 6200: loss 0.526434
iteration 6300: loss 0.524913
iteration 6400: loss 0.523390
iteration 6500: loss 0.526750
iteration 6600: loss 0.526724
iteration 6700: loss 0.527507
iteration 6800: loss 0.524641
iteration 6900: loss 0.527918
iteration 7000: loss 0.522015
iteration 7100: loss 0.524246
iteration 7200: loss 0.521867
iteration 7300: loss 0.533649
iteration 7400: loss 0.526186
iteration 7500: loss 0.528806
iteration 7600: loss 0.529881
iteration 7700: loss 0.521365
iteration 7800: loss 0.563818
iteration 7900: loss 0.525993
iteration 8000: loss 0.521783
iteration 8100: loss 0.531006
iteration 8200: loss 0.530109
iteration 8300: loss 0.523841
iteration 8400: loss 0.528784
iteration 8500: loss 0.520853
iteration 8600: loss 0.536573
iteration 8700: loss 0.521490
iteration 8800: loss 0.594252
iteration 8900: loss 0.528764
iteration 9000: loss 0.526214
iteration 9100: loss 0.529842
iteration 9200: loss 0.525730
iteration 9300: loss 0.526729
iteration 9400: loss 0.521471
iteration 9500: loss 0.524806
iteration 9600: loss 0.530549
iteration 9700: loss 0.525479
iteration 9800: loss 0.521507
iteration 9900: loss 0.534089
iteration 10000: loss 0.526542
iteration 10100: loss 0.522310
iteration 10200: loss 0.527367
iteration 10300: loss 0.521011
iteration 10400: loss 0.525306
iteration 10500: loss 0.521426
iteration 10600: loss 0.528122
iteration 10700: loss 0.531898
iteration 10800: loss 0.530185
iteration 10900: loss 0.524655
iteration 11000: loss 0.527299
iteration 11100: loss 0.520829
iteration 11200: loss 0.524848
iteration 11300: loss 0.529215
iteration 11400: loss 0.549213
iteration 11500: loss 0.536092
iteration 11600: loss 0.527357
iteration 11700: loss 0.528392
iteration 11800: loss 0.521353
iteration 11900: loss 0.524219
training accuracy: 0.93



 ![image](http://img.blog.csdn.net/20160216134631313)
